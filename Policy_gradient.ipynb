{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy gradient.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdhwa7sioj31CtDgZEpFxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abogaziah/Pong_DQN/blob/master/Policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfVTU9Wl_-ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "from torch.distributions.categorical import Categorical\n",
        "import statistics\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woRgDZ8lAYDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class policy(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    super(policy, self).__init__()\n",
        "    self.fc1 = torch.nn.Linear(D_in,H)\n",
        "    self.fc2 = torch.nn.Linear(H,D_out)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = F.softmax(self.fc2(x),dim=0)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NqnOQAm1bIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "din, h, dout = env.observation_space.shape[0], 128 , env.action_space.n\n",
        "policy = policy(din, h, dout)\n",
        "episodes= 500\n",
        "reward_buffer=[]\n",
        "for episode in range(episodes):\n",
        "  state = env.reset()\n",
        "  done = False \n",
        "  rewards=[]\n",
        "  prob_buffer=[]\n",
        "  while not done:\n",
        "    probs = policy.forward(torch.from_numpy(state).float())\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    state, reward, done, _ = env.step(action.item())\n",
        "    rewards.append(reward)\n",
        "    loss = -m.log_prob(action) * reward\n",
        "    loss.backward()\n",
        "  reward_buffer.append(statistics.mean(rewards))\n",
        "plt.plot(reward_buffer)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YesqvXqyC_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}